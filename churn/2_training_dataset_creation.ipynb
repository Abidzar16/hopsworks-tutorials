{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "\n",
    "In this notebook, we will create the actual dataset that we will train our model on. In particular, we will:\n",
    "1. Select the features we want to train our model on.\n",
    "2. Specify how the features should be preprocessed.\n",
    "3. Create a dataset split for training and validation data.\n",
    "\n",
    "![tutorial-flow](images/create_training_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsfs\n",
    "\n",
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "We start by selecting all the features we want to include for model training/inference. In this case, we'll use all features except for the customer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature group.\n",
    "customer_info_fg = fs.get_feature_group(\"customer_info\")\n",
    "\n",
    "# Select features for training data.\n",
    "ds_query = customer_info_fg.select_except([\"customerid\"])\n",
    "\n",
    "ds_query.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Functions\n",
    "\n",
    "We will preprocess our data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this we simply define a mapping between our features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformation functions.\n",
    "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "label_encoder = fs.get_transformation_function(name=\"label_encoder\")\n",
    "\n",
    "numerical_features = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "categorical_features = [\n",
    "    \"MultipleLines\", \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\",\n",
    "    \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\",\n",
    "    \"Contract\", \"PaymentMethod\"]\n",
    "\n",
    "# Features names are lower case in the feature group.\n",
    "numerical_features = [s.lower() for s in numerical_features]\n",
    "categorical_features = [s.lower() for s in categorical_features]\n",
    "\n",
    "# Map features to transformations.\n",
    "transformation_functions = {}\n",
    "for feature in numerical_features:\n",
    "    transformation_functions[feature] = min_max_scaler\n",
    "\n",
    "# TODO there seems to be some problems with label_encoder when having multiple categorical features...\n",
    "# for feature in categorical_features:\n",
    "#     transformation_functions[feature] = label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Creation\n",
    "\n",
    "Finally we create the dataset using `fs.create_training_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = fs.create_training_dataset(\n",
    "    name=\"churn_dataset_splitted\",\n",
    "    label=[\"churn\"],\n",
    "    data_format=\"csv\",\n",
    "    transformation_functions=transformation_functions,\n",
    "    splits={'train': 70, 'validation': 30},\n",
    "    train_split=\"train\"\n",
    ")\n",
    "\n",
    "# We can save the dataset using the query alone.\n",
    "td.save(ds_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sanity check that the transformation functions have been applied by loading the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.read(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.read(\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "In the next notebook, we will train a model on the dataset we created in this notebook."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
